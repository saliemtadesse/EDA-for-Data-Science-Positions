# The first step is obtain the node and nodes of the variable that we are interested in using the selectorGadget 
# and then create a name for the elements as described below.

CSSjobct <- ".jobsCount"
CSSjobtitle <- ".jobHeader+ .jobTitle"
CSScompany <- ".jobEmpolyerName"
CSScitystate <- "#MainCol .loc"
CSSrating <- ".compactStars"
CSSsalary <- ".salary"
CSStime <- ".css-65p68w"
CSSsize <- ".infoEntity:nth-child(2)"

# To avoid the difficulty of searching across the different states, the link for each state was 
# compiled in to a single text file and then was converted in to a list.
URLstate <- read_tsv("Glassdoor-State-Link.txt", col_names = F)
URLstate <- as.list(URLstate$X1)
URLjob <- paste0(URLstate,".htm")

#Finding the number of total jobs per state search
totJobs <- lapply(URLjob, function(i){
  read_html(i) %>%
    html_nodes(".jobsCount") %>%
    html_text() %>%
    gsub("[^0-9]","",.) %>%  ### removing all the numbers from the link
    as.integer()
})

# 30 jobs per pages, want to max at 30 total.
totPages <- lapply(totJobs,function(i){as.integer(min(ceiling(i/30),30))})
rm(totJobs)

# Loop along each state link to create the pages link
URLall <- lapply(seq_along(URLstate), function(i) paste0(URLstate[[i]], "_IP", seq_len(totPages[[i]]),".htm"))
URLall <- unlist(URLall)
URLall

### Pull the individual elements into a dataframe
GD_Scraper <-function(x=1:100) {map_df(x, function(i) {
  cat(" P", i, sep = "")
  pg <- read_html(GET(URLall[i]))
  jobTitle <- pg %>% html_nodes(CSSjobtitle) %>% 
    html_text() %>% data.frame(Job_Title = ., stringsAsFactors = F)
  
  company <- pg %>%  html_nodes(CSScompany) %>% 
    html_text() %>% data.frame(Company_Name = ., stringsAsFactors = F)
  
  cityState <- pg %>%  html_nodes (CSScitystate) %>% 
    html_text() %>% data.frame(City_State = ., stringsAsFactors = F)
  
  rating <- pg %>% html_nodes(".jl") %>% html_node(CSSrating) %>% 
    html_text() %>% data.frame(Company_Rating = ., stringsAsFactors = F)
  
  salary <- pg %>% html_nodes(".jl") %>% html_node(CSSsalary) %>% 
    html_text() %>% data.frame(salary_Range = ., stringsAsFactors = F)
  
  length <- pg %>% html_nodes(".jl") %>% html_node(CSStime) %>% 
    html_text() %>% data.frame(Post_Date = ., stringsAsFactors = F)

  data.raw<- cbind(jobTitle,company,cityState,rating,salary,length)
}) }


# Do the scrapping in parts to prevent errors
data.raw1 <- GD_Scraper(1:100)
data.raw2 <- GD_Scraper(101:200)
data.raw3 <- GD_Scraper(201:300)
data.raw4 <- GD_Scraper(301:400)
data.raw5 <- GD_Scraper(401:500)
data.raw6 <- GD_Scraper(501:length(URLall))

data.total <- data.raw1 %>% 
  rbind(data.raw2,data.raw3,data.raw4,data.raw5,data.raw6)
View(data.total)
saveRDS(data.total,"Data-Raw-Total.RDS")
rm(data.raw1,data.raw2,data.raw3,data.raw4,data.raw5,data.raw6)
Final <- read_rds("Data-Raw-Total.RDS")
